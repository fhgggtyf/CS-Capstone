# This script is unused as of 2024-06-10 but kept for reference.
# It combines multiple sentiment-filtered tables into unified tables per sentiment version.
import sqlite3
import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta, timezone

# === CONFIGURATION ===
# Path to the SQLite database containing the sentiment‑augmented tables.  This file
# is produced by the individual sentiment filtering scripts (e.g.,
# filter_sentiment_emotion.py).  The combine logic will read from this
# database and write the combined sentiment tables back into it.
DB_PATH = "Data_Extraction/Database/CS_Capstone_Sentiment.db"

# In the original non‑sentiment version of this script the combined output table
# was fixed as ``frustrated_reviews_sentiment_combined`` and included a
# ``matched_categories`` column.  For the sentiment‑aware version we no longer
# include that column, and instead we generate one combined table per sentiment
# version.  The new target table names are constructed dynamically based on
# the version prefixes defined below.

# Columns that we always include in the combined table, regardless of sentiment
# version.  ``matched_categories`` is intentionally omitted per user
# instructions.
BASE_FINAL_COLS = ["main_text", "comment_platform", "time", "game_name"]

# Mapping of sentiment version prefixes to the extra sentiment columns that
# should be preserved when combining tables.  The keys correspond to the
# prefixes of individual table names generated by the sentiment filtering
# scripts.  The values list the column names that exist in those tables and
# should be copied verbatim into the combined result.
VERSION_EXTRA_COLS = {
    # Emotion‑based sentiment analysis.  Each input table with this prefix
    # contains these columns describing the dominant emotion and its details.
    "frustrated_sentiment_emotions": [
        "dominant_emotion",
        "dominant_score",
        "top_emotions",
        "emotions_json",
        "final_label",
    ],
    # Binary sentiment classification (positive/negative) v1.  Input tables
    # contain two columns summarising the model’s label and confidence.
    "frustrated_sentiment_pos_neg_v1": [
        "sentiment_label",
        "sentiment_score",
    ],
    # Hybrid sentiment classification v2.  Input tables include these
    # additional columns from the transformer and VADER analysis.
    "frustrated_sentiment_pos_neg_v2": [
        "final_label",
        "transformer_label",
        "transformer_score",
        "vader_compound",
    ],
}

# Columns that might hold the primary text (first hit wins, aliased to main_text)
MAIN_TEXT_CANDIDATES = [
    "main_text", "body", "body_text", "content", "text", "post_title",
    "review_text", "review", "comment", "message", "post_text", "description"
]

# Time candidates we’ll scan to derive a normalized date
TIME_CANDIDATES = [
    "time", "time_str", "created_utc", "created_at", "published_at", "updated_at",
    "date", "datetime", "timestamp_created", "post_time", "post_date", "posted_at",
    "review_date", "reply_time", "review_time", "comment_time", "comment_date",
    "scraped_at", "fetched_at"
]

# Accept several explicit time formats before falling back to pandas parser.
# This expanded list of explicit time formats covers many common
# representations encountered in game comments, including fractional
# seconds, slashes instead of dashes, various day/month orders, and
# partial timestamps. New entries can be appended here to catch
# additional custom formats.
KNOWN_TIME_FORMATS = [
    # ISO-like with spaces
    "%Y-%m-%d %H:%M:%S",         # 2024-09-01 12:34:56
    "%Y-%m-%d %H:%M:%S.%f",     # 2024-09-01 12:34:56.123456
    "%Y-%m-%d %H:%M:%S%z",      # 2024-09-01 12:34:56+0000 or +00:00
    "%Y-%m-%d %H:%M:%S.%f%z",  # 2024-09-01 12:34:56.123456+0000
    "%Y-%m-%d %H:%M",           # 2024-09-01 12:34
    "%Y-%m-%d",                 # 2024-09-01
    # ISO-like with T separator
    "%Y-%m-%dT%H:%M:%S",        # 2024-09-01T12:34:56
    "%Y-%m-%dT%H:%M:%S.%f",    # 2024-09-01T12:34:56.123
    "%Y-%m-%dT%H:%M:%SZ",       # 2024-09-01T12:34:56Z
    "%Y-%m-%dT%H:%M:%S.%fZ",   # 2024-09-01T12:34:56.123Z
    "%Y-%m-%dT%H:%M:%S%z",      # 2024-09-01T12:34:56+0000 or +00:00
    "%Y-%m-%dT%H:%M:%S.%f%z",  # 2024-09-01T12:34:56.123456+0000
    "%Y-%m-%dT%H:%M",           # 2024-09-01T12:34
    # Slash-separated with time
    "%Y/%m/%d %H:%M:%S",         # 2024/09/01 12:34:56
    "%Y/%m/%d %H:%M:%S.%f",     # 2024/09/01 12:34:56.123456
    "%Y/%m/%d %H:%M",           # 2024/09/01 12:34
    "%Y/%m/%d",                  # 2024/09/01
    "%m/%d/%Y %H:%M:%S",         # 09/01/2024 12:34:56 (US ordering)
    "%m/%d/%Y %H:%M:%S.%f",     # 09/01/2024 12:34:56.123456
    "%m/%d/%Y %H:%M",           # 09/01/2024 12:34
    "%m/%d/%Y",                  # 09/01/2024
    "%d/%m/%Y %H:%M:%S",         # 01/09/2024 12:34:56 (European ordering)
    "%d/%m/%Y %H:%M:%S.%f",     # 01/09/2024 12:34:56.123456
    "%d/%m/%Y %H:%M",           # 01/09/2024 12:34
    "%d/%m/%Y",                  # 01/09/2024
    # Dash-separated with day-month-year and month-day-year
    "%d-%m-%Y %H:%M:%S",         # 01-09-2024 12:34:56
    "%d-%m-%Y %H:%M:%S.%f",     # 01-09-2024 12:34:56.123456
    "%d-%m-%Y %H:%M",           # 01-09-2024 12:34
    "%d-%m-%Y",                  # 01-09-2024
    "%m-%d-%Y %H:%M:%S",         # 09-01-2024 12:34:56
    "%m-%d-%Y %H:%M:%S.%f",     # 09-01-2024 12:34:56.123456
    "%m-%d-%Y %H:%M",           # 09-01-2024 12:34
    "%m-%d-%Y",                  # 09-01-2024
    # Textual month names
    "%b %d, %Y",                 # Sep 1, 2024
    "%d %b %Y",                  # 01 Sep 2024
    # RFC-like formats
    "%a, %d %b %Y %H:%M:%S %Z",  # Mon, 05 Jul 2021 12:34:56 GMT
    "%a %b %d %H:%M:%S %Y",      # Mon Jul 5 12:34:56 2021
    # Dot-separated
    "%Y.%m.%d",                  # 2024.09.01
    "%Y.%m.%d %H:%M:%S",         # 2024.09.01 12:34:56
    "%Y.%m.%d %H:%M:%S.%f",     # 2024.09.01 12:34:56.123456
    # Compact numeric formats
    "%Y%m%d",                    # 20240901
    "%Y%m%d%H%M",               # 202409011234 (lack seconds)
    "%Y%m%d%H%M%S",              # 20240901123456
    "%Y%m%d%H%M%S%f",           # 20240901123456123
    "%Y%m%d%H%M%S.%f"           # 20240901123456.123 (rare)
]

# Normalized (punctuation/space/underscore-agnostic) game map
GAME_NAME_MAP = {
    "assasinscreedunity": "Assasin's Creed Unity",
    "assassinscreedunity": "Assasin's Creed Unity",
    "cyberpunk2077": "Cyberpunk 2077",
    "sekiroshadowsdietwice": "Sekiro: Shadows Die Twice",
    "leagueoflegends": "League of Legends",
    "escapefromtarkov": "Escape From Tarkov",
    "crashbandicoot4itsabouttime": "Crash Bandicoot 4: It’s About Time",
    "crash4": "Crash Bandicoot 4: It’s About Time",
    "darksoulsremastered": "Dark Souls (Remastered)",
    "cuphead": "Cuphead",
    "supermeatboy": "Super Meat Boy",
    "devilmaycry5": "Devil May Cry 5",
    "battlefield4": "Battlefield 4",
    "wwe2k20": "WWE 2K20",
    "halothemasterchiefcollection": "Halo: The Master Chief Collection",
    "halomcc": "Halo: The Master Chief Collection",
    "themasterchiefcollection": "Halo: The Master Chief Collection",
    "fallout76": "Fallout 76",
    "masseffectandromeda": "Mass Effect: Andromeda",
    "battlefield2042": "Battlefield 2042",
    "batmanarkhamknight": "Batman: Arkham Knight",
    "anthem": "Anthem",
    "nomanssky": "No Man's Sky",
    "starwarsbattlefrontii": "Star Wars Battlefront II",
    "simcity2013": "SimCity (2013)",
    "mightyno9": "Mighty No. 9",
    "marvelsavengers": "Marvel's Avengers",
    "tonyhawkride": "Tony Hawk: Ride",
    "fortnite": "Fortnite",
    "dota2": "DOTA 2",
    "rainbow6siege": "Rainbow 6: Siege",
    "rainbowsixsiege": "Rainbow 6: Siege",
    "tomclancysrainbow6siege": "Rainbow 6: Siege",
    "bindingofisaacrepentance": "The Binding of Isaac: Repentance",
    "gettingoveritwithbennettfoddy": "Getting Over It with Bennett Foddy",
    "furi": "Furi",
    "jumpking": "Jump King",
    "mortalshell": "Mortal Shell",
    "streetfighterv": "Street Fighter V",
    "counterstrike2": "Counter_Strike 2",
    "apexlegends": "APEX Legends",
    "genshinimpact": "Genshin Impact",
    "callofdutywarzone": "Call of Duty: Warzone",
    "valorant": "Valorant",
    "hogwartslegacy": "Hogwarts Legacy",
    "palworld": "PalWorld",
    "baldursgate3": "Baldur's Gate 3",
    "baldursgate": "Baldur's Gate 3",
    "thelordoftheringsgollum": "The Lord of the Rings: Gollum",
    "callofdutymodernwarfareiii": "Call of Duty: Modern Warfare III",
    "warcraftiiireforged": "Warcraft III: Reforged",
    "pubgbattlegrounds": "PUBG: Battlegrounds",
    "pubg": "PUBG: Battlegrounds",
    "warthunder": "War Thunder",
    "helldivers2": "Helldivers 2",
    "overwatch2": "Overwatch 2",
    "deltaforce": "Delta Force (2024)",
    "easportsfc25": "EA SPORTS FC™ 25",
    "fc25": "EA SPORTS FC™ 25",
    "easports25": "EA SPORTS FC™ 25",
}

# ---------- helpers ----------

def normalize_name(s: str) -> str:
    """Lowercase and remove all non-alphanumeric to normalize names/keys."""
    return re.sub(r"[^a-z0-9]", "", s.lower())

def get_game_name(table_name: str) -> str:
    norm = normalize_name(table_name)
    for key, game in GAME_NAME_MAP.items():
        if key in norm:
            return game
    return table_name  # fallback if no match

# Infer a simple comment platform from the table name.
# Only four platforms are considered: 'reddit', 'metacritic', 'steam', and 'official'.
# If none of these keywords are present in the table name, returns None.
from typing import Optional

def get_platform_name(table_name: str) -> Optional[str]:
    """
    Derive the comment platform from the table name by searching for
    known platform keywords (reddit, metacritic, steam, official). Case-insensitive.

    Parameters
    ----------
    table_name : str
        The name of the table being processed.

    Returns
    -------
    Optional[str]
        One of 'reddit', 'metacritic', 'steam', 'official' if the table name
        contains the corresponding substring; otherwise None.
    """
    name = table_name.lower()
    if "reddit" in name:
        return "reddit"
    elif "metacritic" in name:
        return "metacritic"
    elif "steam" in name:
        return "steam"
    elif "official" in name:
        return "official"
    return None

def alias_first_present(table_cols: set, candidates: list, alias: str) -> str:
    """Return SELECT fragment mapping first candidate present to alias; else NULL AS alias."""
    for c in candidates:
        if c in table_cols:
            return f"{c} AS {alias}"
    return f"NULL AS {alias}"

def _from_epoch_auto_scale(x: float) -> datetime:
    """
    Convert epoch value to UTC datetime with automatic unit detection:
    seconds (~1e9), milliseconds (~1e12), microseconds (~1e15), nanoseconds (~1e18).
    """
    if x > 1e18:
        x /= 1e9
    elif x > 1e15:
        x /= 1e6
    elif x > 1e12:
        x /= 1e3
    # else seconds
    return datetime.fromtimestamp(x, tz=timezone.utc)

def _try_parse_known_formats(s: str):
    for fmt in KNOWN_TIME_FORMATS:
        try:
            return datetime.strptime(s, fmt).replace(tzinfo=timezone.utc)
        except Exception:
            pass
    return None

def parse_any_datetime(val):
    """
    Return timezone-aware UTC datetime or np.nan.
    Handles: epoch sec/ms/µs/ns, numeric strings, common date strings, pandas Timestamp.
    """
    if val is None or (isinstance(val, float) and np.isnan(val)):
        return np.nan

    # Pandas Timestamp
    try:
        if isinstance(val, pd.Timestamp):
            return (val.tz_localize("UTC") if val.tzinfo is None else val.tz_convert("UTC")).to_pydatetime()
    except Exception:
        pass

    # Pure numbers: epoch
    if isinstance(val, (int, float)) and not isinstance(val, bool):
        try:
            return _from_epoch_auto_scale(float(val))
        except Exception:
            return np.nan

    # Strings
    if isinstance(val, str):
        s = val.strip()
        if not s:
            return np.nan
        # Numeric string: may represent epoch seconds/ms/us/ns or compact date/time
        if re.fullmatch(r"[+-]?\d+(\.\d+)?", s):
            # Only treat pure integers here (no decimal part)
            if '.' not in s:
                digits = s.lstrip('+-')
                # Try interpreting certain lengths as compact date/time
                try:
                    if len(digits) == 8:
                        # YYYYMMDD
                        dt = datetime.strptime(digits, "%Y%m%d").replace(tzinfo=timezone.utc)
                        return dt
                    elif len(digits) == 12:
                        # YYYYMMDDHHMM (no seconds)
                        dt = datetime.strptime(digits, "%Y%m%d%H%M").replace(tzinfo=timezone.utc)
                        return dt
                    elif len(digits) == 14:
                        # YYYYMMDDHHMMSS
                        dt = datetime.strptime(digits, "%Y%m%d%H%M%S").replace(tzinfo=timezone.utc)
                        return dt
                    elif len(digits) == 17:
                        # YYYYMMDDHHMMSSfff (milliseconds or truncated microseconds)
                        date_part = digits[:14]
                        frac_part = digits[14:]
                        frac_micro = (frac_part + '000000')[:6]
                        dt = datetime.strptime(date_part + frac_micro, "%Y%m%d%H%M%S%f").replace(tzinfo=timezone.utc)
                        return dt
                    # For other lengths, fall through to epoch conversion
                except Exception:
                    # If parsing as date fails, fall back to epoch
                    pass
            # Treat numeric string as epoch (seconds/ms/us/ns)
            try:
                return _from_epoch_auto_scale(float(s))
            except Exception:
                pass
        # Try explicit formats first
        dt = _try_parse_known_formats(s)
        if dt is not None:
            return dt
        # Try pandas to_datetime for a variety of formats
        try:
            p = pd.to_datetime(s, utc=True, errors="coerce")
            if not pd.isna(p):
                return p.to_pydatetime()
        except Exception:
            pass
        # Final fallback: dateutil parser handles many miscellaneous formats
        try:
            from dateutil import parser as du_parser
            dt = du_parser.parse(s)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            else:
                dt = dt.astimezone(timezone.utc)
            return dt
        except Exception:
            return np.nan

    return np.nan

def best_datetime_series(df: pd.DataFrame, candidates: list) -> pd.Series:
    """
    Parse per-candidate time columns and return the row-wise first non-null UTC datetime.
    """
    parsed_cols = []
    for col in candidates:
        if col in df.columns:
            parsed_cols.append(df[col].apply(parse_any_datetime))
    if not parsed_cols:
        return pd.Series([pd.NaT] * len(df), index=df.index, dtype="datetime64[ns, UTC]")

    combined = parsed_cols[0]
    for s in parsed_cols[1:]:
        combined = combined.where(pd.notna(combined), s)
    return pd.to_datetime(combined, utc=True, errors="coerce")

# ---------- main combine ----------

# Open a single connection up front.  We'll reuse it for reading from and
# writing to the database.  Closing the connection happens at the end of
# processing all sentiment versions.
conn = sqlite3.connect(DB_PATH)
cur = conn.cursor()
cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
all_names = [r[0] for r in cur.fetchall()]

# The original script merged all ``frustrated`` tables into a single target.
# For sentiment analysis we need to perform separate merges per version
# prefix defined in VERSION_EXTRA_COLS.  Each combined table will be named
# ``{version_prefix}_sentiment_combined`` and will include the base set of
# columns plus any extra columns associated with the version.
for version_prefix, extra_cols in VERSION_EXTRA_COLS.items():
    # Determine the name of the output table for this version.
    target_name = f"{version_prefix}_sentiment_combined"

    # Identify all tables whose names start with the current version prefix.
    # We exclude sqlite's internal tables.  Note: ``startswith`` is used
    # instead of a substring search to avoid accidentally merging unrelated
    # tables containing the version prefix in the middle of the name.
    input_tables = [
        n for n in all_names
        if n.lower().startswith(version_prefix.lower())
        and not n.lower().startswith("sqlite_")
    ]

    all_rows = []

    for table in input_tables:
        game_name = get_game_name(table)
        # Derive platform based on keywords in table name. Only known platforms are considered.
        platform_name = get_platform_name(table)

        # Inspect schema
        info = pd.read_sql_query(f"PRAGMA table_info({table})", conn)
        table_cols = set(info["name"].tolist())

        # Build SELECT:
        #  - main_text from first candidate (aliased to main_text)
        #  - include any requested extra sentiment columns (NULL if absent)
        #  - include all present time-candidate columns so we can parse them
        main_text_sel = alias_first_present(table_cols, MAIN_TEXT_CANDIDATES, "main_text")

        # Time columns: select only those that physically exist in the table.
        time_sel = [c for c in TIME_CANDIDATES if c in table_cols]

        # Sentiment columns: ensure every column requested for this version is selected.
        # If a column is missing, select NULL AS <col> to preserve the schema.
        sentiment_sel = [
            (col if col in table_cols else f"NULL AS {col}")
            for col in extra_cols
        ]

        # We do not select comment_platform here because we override it after
        # reading.  Similarly we omit matched_categories entirely.

        select_parts = [main_text_sel] + sentiment_sel + time_sel
        select_sql = ", ".join(select_parts)

        df = pd.read_sql_query(f"SELECT {select_sql} FROM {table}", conn)

        # Parse time from any plausible columns and filter by per-table 3-year window
        parsed = best_datetime_series(df, TIME_CANDIDATES)

        if parsed.notna().any():
            latest_dt = parsed.max()  # pandas Timestamp[UTC]
            cutoff_dt = latest_dt - pd.Timedelta(days=3 * 365)
            keep = parsed >= cutoff_dt
            df = df.loc[keep].copy()
            # Format the time as ISO date (YYYY-MM-DD) in UTC
            df["time"] = parsed.loc[keep].dt.strftime("%Y-%m-%d").values
        else:
            # If no valid timestamps are found, still include rows but set time to None
            df["time"] = None

        # Append game name and platform name for each row
        df["game_name"] = game_name
        # Override or set the comment platform for every row based on table name.
        # This intentionally ignores any existing comment_platform values from the database to
        # ensure consistent platform tagging for the merged table.
        df["comment_platform"] = platform_name

        if not df.empty:
            # Ensure consistent column order: base columns first, then extra sentiment columns.
            final_cols = BASE_FINAL_COLS + extra_cols
            df = df.reindex(columns=final_cols)
            all_rows.append(df)

    # After processing all tables for this version, combine and write the result
    if all_rows:
        combined = pd.concat(all_rows, ignore_index=True)
        combined.to_sql(target_name, conn, if_exists="replace", index=False)
        print(f"✅ Combined table saved as '{target_name}' with {len(combined)} rows.")
    else:
        print(f"No data found to combine for version '{version_prefix}'.")

# Close the connection when finished
conn.close()